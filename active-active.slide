Active-Active
Using Multi-leader Application Architecture for Fun and Profit
9 Aug 2019
Tags: distributed-systems, go, aws

Mark Thomas
Senior Software Engineer, Oberon, FloQast
markt@ifelse.io
https://ifelse.io
@markthethomas

* Agenda
- intro
- sample application using active-active on AWS
- code/infra walkthrough(s)
- (hopefully functioning) demo

* Let's talk about replication

* not *that* replication

.image ./images/armadillo.gif _ 500

* Database replication

* 
The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repair. 

Douglas Adams, _Mostly_ _Harmless_ (1992)

* ...oops
.image ./images/elmo-fire.gif _ 700

* Replication

"getting the same data elsewhere"

why? 

1. latency: close to users
2. availablity: work when things go wrong
3. throughput: do more stuff

: talk a little bit about why its important


* Active...active?

    systems that support authortitative writes to multiple nodes

also "multi-leader"

as opposed to *active-passive*:

- primary/secondary
- all writes go through primary, replicated to secondary/ies
- active-active definition not universally agreed on

: active/active is one type of replication

* Some examples...

* leader-based
.image ./images/leader-based-replication.png _ 980

* leader-based (sync/async)
.image ./images/sync-async-leader-replication.png _ 980

* multi-leader
.image ./images/multi-leader.png _ 1000

* What's it good for? 

- mutli-datacenter operation
- maximum availablity/survivability
- offline-clients
- user-localized compute

: talk about some use cases and why its not for everyone

* Multi-leader Tradeoffs

* The bad...

- hard/complex to set up for most databases
- conflict resolution is hard
- "easy" things can be hard: autoincrementing keys, constraints, &c.

: most databases don't support this out of the box because its hard and not something everyone needs
: conflict resolution can be really hard to implement correctly because its a fundamental distributed systems problem of consensus

* conflict resolution
.image ./images/failed-conflict-resolution.png _ 950

* 
.image ./images/elmo-fire.gif _ 700

* GitHub incident 09/2012

Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents. For example, in one incident at GitHub , an out-of-date MySQL follower was promoted to leader. The database used an autoincrementing counter to assign primary keys to new rows but because the new leader’s counter lagged behind the old leader’s, it reused some primary keys that were previously assigned by the old leader. These primary keys were also used in a Redis store, so the reuse of primary keys resulted in inconsistency between MySQL and Redis, which caused some private data to be disclosed to the wrong users.

(from Designing Data-Intensive Applications)

.link https://github.blog/2012-09-14-github-availability-this-week/ incident summary

* 
.image ./images/elmo-fire.gif _ 700

* ...the "just how it is"...
- async replication
- eventually-globally consistent

: speed of light is a real factor; replication takes some time
: a consistent view of data globally will eventually be possible after replication across all nodes

* ...and the good!

- multi-datacenter/device
- multiple primaries

which means:

- performance
- network resilience
- fault-tolerance
- right-sizing
- recovery time

: lower latency for reads; read/write load distributed across nodes
: network: writes through many network paths instead of just one
: AZ/DC outage survivability
: possible to size capacity at multiple points instead of just one; aka maybe one region gets hot, they don't all have to
: recovery time: potentially near-immediate fail-over w/ minimal intervention

* Sounds cool! Let's build...

* FloQars

* 
.image ./images/floqars-logo.png _ 900

* FloQars
Main goal: uber/lyft killer, global domination, etc.

requirements:

1. low-latency for regional users around the world
2. read/write load varies by region
3. high fault-tolerance
4. help accounts close faster by enabling them to work while commuting
5. ideally Excel-based configuration

* Possible solution
Use DynamoDB, API Gateway, Lambda, Go, and Route 53 to create a multi-master, mutli-region application.

.image ./images/multi-region-arch.png

* System components

- DNS + failover: *Route53* w/ latency-based routing & health check
- serving layer: *API* *gateway* backed by *lambdas* running Go 
- data layer: *dynamoDB* using global tables

which gives us...

- auto-failover at the network level
- automatic replication catchup after outage resolution
- lower latency due to geo-routing & data ubiquity
- multi-region protection for disaster recovery

* Architecture component overview

* DynamoDB
- wide-column NoSQL store; distributed hash table ([[https://en.wikipedia.org/wiki/Distributed_hash_table][DHT]])
- tunable read/write consistency (more consistency == more $)
- [[https://en.wikipedia.org/wiki/Apache_Cassandra][Cassandra]]-ish & same parents (_Avinash_ _Lakshman_)
- global tables: replciated tables
- secondary indexes
- streams
- DAX (dynamo DB accelerator) for even lower latency

dynamoDB on the DB spectrum

    rocksDB -> redis -> dynamo -> mongo -> postgres
    <--------------------------------------------->
    k/v        k/v++      DHT       DHT++      RDMS

: dynamo is great for high-read scenarios where you really understand your access patterns. 
: redis -> dynamo -> mongo -> postgres
: you can build basically anything w/ a DHT - but you have to do more work to get certain things

* Lambda
- lightweight compute
- integrates with API gateway
- can also used as API authorizer

* API Gateway
- Product and Pattern
- Connect X to Y
- Where *Y* is { lambda, ec2, kinesis, aws endpoints, ecs, dynamoDB }
- more: throttling, API keys, doc gen, SDK gen
: why use this? easy regional API deployment

* Route 53
- highly-available DNS service
- health-checks & types of routing
- latency-based vs. geo-based

: We'll use Route 53 to handle shifting our traffic around at the DNS level. Each FloQars region will reply with a health check and R53 will use that for routing decisions
: Route53 has a "100%" SLA, so in theory its the best place to shift traffic around failing systems

* Go
- statically-typed
- memory-safety
- gargbage-collected
- structural typing
- CSP-style concurrency
.link https://golang.org/ language site

.image ./images/Go-Logo_Blue.png _ 300

* Review...
.image ./images/multi-region-arch.png

* Creating Our API Gateway + Lambdas...
.link https://serverless.com/ serverless framework
.play ./code/active-active/serverless.yml

* Creating API Gateway...
- regional endpoints vs. edge-optimized
- lambda proxy integration
- custom domain name

.link https://latency.apex.sh/?url=https%3A%2F%2Fapi.floqars.com%2Fhealthy&compare= Let's check the latency!

: go to the console

* Creating the Health check lambda
.code -edit -numbers ./code/active-active/healthcheck.go /1 OMIT/,/1 OMIT/

* Health check lambda (cont.)
.code -edit -numbers ./code/active-active/healthcheck.go /2 OMIT/,/2 OMIT/

* Health check lambda (cont.)
.code -edit -numbers ./code/active-active/healthcheck.go /3 OMIT/,/3 OMIT/

* Setting up the health checks...

: go to the console

* Failing over!
: go to the console
: update the region health
: keep using the API to create records


* Recap
.image ./images/multi-region-arch.png

* Any questions?

* Credits/Images/etc.
.link https://amzn.to/2YPrSt7 Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems
.link https://read.acloud.guru/location-based-search-results-with-dynamodb-and-geohash-267727e5d54f Location-based search results with DynamoDB and Geohash
.link https://www.mongodb.com/blog/post/active-active-application-architectures-with-mongodb Active-Active Application Architectures with MongoDB
.link https://read.acloud.guru/building-a-serverless-multi-region-active-active-backend-36f28bed4ecf Build a serverless multi-region, active-active backend solution in an hour
.link https://read.acloud.guru/why-and-how-do-we-build-a-multi-region-active-active-architecture-6d81acb7d208 How to build a multi-region active-active architecture on AWS
